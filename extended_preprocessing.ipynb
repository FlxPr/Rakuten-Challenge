{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from pandarallel import pandarallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 4 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "pandarallel.initialize(progress_bar = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('X_train_update.csv', index_col = 0)\n",
    "df_test = pd.read_csv('X_test_update.csv', index_col = 0)\n",
    "y_train = pd.read_csv('Y_train_CVw08PX.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(i, df_train.designation[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First of all, let's identify the language for each product\n",
    "Because of long computation time, results have been saved in a csv file.\n",
    "See below correponding code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def language_detection(df):\n",
    "    \n",
    "    from spacy_langdetect import LanguageDetector\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    nlp.add_pipe(LanguageDetector(), name='language_detector', last=True)\n",
    "\n",
    "    df['language_designation'] = df['designation'].parallel_apply(\n",
    "        lambda x : nlp(x)._.language)\n",
    "    \n",
    "    df['language_description'] = df['description'].parallel_apply(\n",
    "        lambda x : {'language': 'unknown', 'score': 0}\n",
    "        if pd.isnull(x)\n",
    "        else\n",
    "        nlp(x)._.language)\n",
    "    \n",
    "    df['language'] = df.parallel_apply(lambda x : \n",
    "                          x['language_designation']['language'] \n",
    "                          if ((x['language_designation']['score'] > x['language_description']['score']) & (x['language_designation']['score'] > 0.95))\n",
    "                          else\n",
    "                          (x['language_description']['language'] if x['language_description']['score'] > 0.9\n",
    "                           else 'fr'),\n",
    "                          axis = 1)\n",
    "    \n",
    "    df.drop(['language_designation', 'language_description'], axis = 1, inplace = True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_detection(df_train)\n",
    "language_detection(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train : \\n',df_train.groupby('language')['productid'].count())\n",
    "print('test : \\n',df_test.groupby('language')['productid'].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Preprocessing \n",
    "### With corresponding language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_fr = spacy.load(\"fr_core_news_sm\")\n",
    "nlp_en = spacy.load(\"en_core_web_sm\")\n",
    "nlp_de = spacy.load(\"de_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_accent(string):\n",
    "    string = string.replace('á', 'a')\n",
    "    string = string.replace('â', 'a')\n",
    "\n",
    "    string = string.replace('é', 'e')\n",
    "    string = string.replace('è', 'e')\n",
    "    string = string.replace('ê', 'e')\n",
    "    string = string.replace('ë', 'e')\n",
    "\n",
    "    string = string.replace('î', 'i')\n",
    "    string = string.replace('ï', 'i')\n",
    "\n",
    "    string = string.replace('ö', 'o')\n",
    "    string = string.replace('ô', 'o')\n",
    "    string = string.replace('ò', 'o')\n",
    "    string = string.replace('ó', 'o')\n",
    "\n",
    "    string = string.replace('ù', 'u')\n",
    "    string = string.replace('û', 'u')\n",
    "    string = string.replace('ü', 'u')\n",
    "\n",
    "    string = string.replace('ç', 'c')\n",
    "    \n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_digits(string):\n",
    "    result = ''.join([i for i in string if not i.isdigit()])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_to_tokens(raw_string, spacy_nlp):\n",
    "    # Write code for lower-casing\n",
    "    string = raw_string.lower()\n",
    "    \n",
    "    string = normalize_accent(string)\n",
    "    \n",
    "    string = remove_digits(string)\n",
    "    \n",
    "    spacy_tokens = spacy_nlp(string)\n",
    "        \n",
    "    string_tokens = [token.orth_ for token in spacy_tokens if not token.is_punct and not token.is_stop]\n",
    "    \n",
    "    clean_string = \" \".join(string_tokens)\n",
    "    \n",
    "    return clean_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_to_tokens_with_language(df):\n",
    "    df['designation_preprocessed'] = df.parallel_apply(\n",
    "        lambda x : raw_to_tokens(x['designation'], nlp_de) if x['language'] == 'de'\n",
    "        else (raw_to_tokens(x['designation'],nlp_en) if x['language'] == 'en' \n",
    "              else raw_to_tokens(x['designation'],nlp_fr)), axis = 1)\n",
    "    \n",
    "    df['description_preprocessed'] = df.parallel_apply(\n",
    "    lambda x : '' if pd.isnull(x['description']) else raw_to_tokens(x['description'], nlp_de) if x['language'] == 'de'\n",
    "    else (raw_to_tokens(x['description'],nlp_en) if x['language'] == 'en' \n",
    "          else raw_to_tokens(x['description'],nlp_fr)), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_to_tokens_with_language(df_train)\n",
    "raw_to_tokens_with_language(df_test)\n",
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spellcheck(df):\n",
    "    from spellchecker import SpellChecker\n",
    "    spell = SpellChecker()\n",
    "    df['designation_ckecked'] = df['designation'].parallel_apply(lambda x : ' '.join([word for word in spell.known(x)]) + ' '.join([spell.correction(word) for word in spell.unknown(x)]))\n",
    "    df['description_ckecked'] = df['description'].parallel_apply(lambda x : np.nan if pd.isnull(x) else ' '.join([word for word in spell.known(x)]) + ' '.join([spell.correction(word) for word in spell.unknown(x)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spellcheck(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks(df):\n",
    "    df['design_noun_chunks'] = df.parallel_apply(\n",
    "    lambda x : ' '.join([chunk.text for chunk in nlp_de(x['designation']).noun_chunks]) if x['language'] == 'de'\n",
    "    else (' '.join([chunk.text for chunk in nlp_en(x['designation']).noun_chunks]) if x['language'] == 'en' \n",
    "          else ' '.join([chunk.text for chunk in nlp_fr(x['designation']).noun_chunks])), axis = 1)\n",
    "    \n",
    "    df['descr_noun_chunks'] = df.parallel_apply(\n",
    "    lambda x : np.nan if pd.isnull(x['description']) else ' '.join([chunk.text for chunk in nlp_de(x['description']).noun_chunks]) if x['language'] == 'de'\n",
    "    else (' '.join([chunk.text for chunk in nlp_en(x['description']).noun_chunks]) if x['language'] == 'en' \n",
    "          else ' '.join([chunk.text for chunk in nlp_fr(x['description']).noun_chunks])), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_chunks(df_train)\n",
    "get_chunks(df_test)\n",
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('df_train_preprocessed_full_2.csv', index = False)\n",
    "df_test.to_csv('df_test_preprocessed_full_2.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from googletrans import Translator\n",
    "translator = Translator()\n",
    "translator.translate(['hello'], dest='fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MYMEMORY WARNING: YOU USED ALL AVAILABLE FREE TRANSLATIONS FOR TODAY. NEXT AVAILABLE IN  01 HOURS 25 MINUTES 26 SECONDSVISIT HTTPS://MYMEMORY.TRANSLATED.NET/DOC/USAGELIMITS.PHP TO TRANSLATE MORE'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from translate import Translator\n",
    "translator= Translator(to_lang=\"fr\")\n",
    "translation = translator.translate(\"This is a pen.\")\n",
    "translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('df_train_preprocessed_full_2.csv')\n",
    "df_test = pd.read_csv('df_test_preprocessed_full_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>designation</th>\n",
       "      <th>description</th>\n",
       "      <th>productid</th>\n",
       "      <th>imageid</th>\n",
       "      <th>language</th>\n",
       "      <th>designation_preprocessed</th>\n",
       "      <th>description_preprocessed</th>\n",
       "      <th>design_noun_chunks</th>\n",
       "      <th>descr_noun_chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Olivia: Personalisiertes Notizbuch / 150 Seite...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3804725264</td>\n",
       "      <td>1263597046</td>\n",
       "      <td>de</td>\n",
       "      <td>olivia personalisiertes notizbuch   seiten pun...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Olivia, / 150 Seiten, / Punktraster, / Ca, Di...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Journal Des Arts (Le) N° 133 Du 28/09/2001 - L...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>436067568</td>\n",
       "      <td>1008141237</td>\n",
       "      <td>fr</td>\n",
       "      <td>journal arts n°   art marche salon art asiatiq...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Journal, Des Arts (, - L'art Et Son Marche Sa...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Grand Stylet Ergonomique Bleu Gamepad Nintendo...</td>\n",
       "      <td>PILOT STYLE Touch Pen de marque Speedlink est ...</td>\n",
       "      <td>201115110</td>\n",
       "      <td>938777978</td>\n",
       "      <td>fr</td>\n",
       "      <td>grand stylet ergonomique bleu gamepad nintendo...</td>\n",
       "      <td>pilot style touch pen marque speedlink   style...</td>\n",
       "      <td>[Grand Stylet Ergonomique Bleu Gamepad Nintend...</td>\n",
       "      <td>[PILOT STYLE, Touch, Pen de marque Speedlink e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Peluche Donald - Europe - Disneyland 2000 (Mar...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50418756</td>\n",
       "      <td>457047496</td>\n",
       "      <td>fr</td>\n",
       "      <td>peluche donald europe disneyland   marionnette...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Peluche Donald, - Europe, Disneyland 2000, À ...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>La Guerre Des Tuques</td>\n",
       "      <td>Luc a des id&amp;eacute;es de grandeur. Il veut or...</td>\n",
       "      <td>278535884</td>\n",
       "      <td>1077757786</td>\n",
       "      <td>fr</td>\n",
       "      <td>guerre tuques</td>\n",
       "      <td>luc id&amp;eacute;es grandeur veut organiser jeu g...</td>\n",
       "      <td>[La Guerre, Des Tuques]</td>\n",
       "      <td>[Luc, des id&amp;eacute;es de grandeur, Il, un jeu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         designation  \\\n",
       "0  Olivia: Personalisiertes Notizbuch / 150 Seite...   \n",
       "1  Journal Des Arts (Le) N° 133 Du 28/09/2001 - L...   \n",
       "2  Grand Stylet Ergonomique Bleu Gamepad Nintendo...   \n",
       "3  Peluche Donald - Europe - Disneyland 2000 (Mar...   \n",
       "4                               La Guerre Des Tuques   \n",
       "\n",
       "                                         description   productid     imageid  \\\n",
       "0                                                NaN  3804725264  1263597046   \n",
       "1                                                NaN   436067568  1008141237   \n",
       "2  PILOT STYLE Touch Pen de marque Speedlink est ...   201115110   938777978   \n",
       "3                                                NaN    50418756   457047496   \n",
       "4  Luc a des id&eacute;es de grandeur. Il veut or...   278535884  1077757786   \n",
       "\n",
       "  language                           designation_preprocessed  \\\n",
       "0       de  olivia personalisiertes notizbuch   seiten pun...   \n",
       "1       fr  journal arts n°   art marche salon art asiatiq...   \n",
       "2       fr  grand stylet ergonomique bleu gamepad nintendo...   \n",
       "3       fr  peluche donald europe disneyland   marionnette...   \n",
       "4       fr                                      guerre tuques   \n",
       "\n",
       "                            description_preprocessed  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2  pilot style touch pen marque speedlink   style...   \n",
       "3                                                NaN   \n",
       "4  luc id&eacute;es grandeur veut organiser jeu g...   \n",
       "\n",
       "                                  design_noun_chunks  \\\n",
       "0  [Olivia, / 150 Seiten, / Punktraster, / Ca, Di...   \n",
       "1  [Journal, Des Arts (, - L'art Et Son Marche Sa...   \n",
       "2  [Grand Stylet Ergonomique Bleu Gamepad Nintend...   \n",
       "3  [Peluche Donald, - Europe, Disneyland 2000, À ...   \n",
       "4                            [La Guerre, Des Tuques]   \n",
       "\n",
       "                                   descr_noun_chunks  \n",
       "0                                                 []  \n",
       "1                                                 []  \n",
       "2  [PILOT STYLE, Touch, Pen de marque Speedlink e...  \n",
       "3                                                 []  \n",
       "4  [Luc, des id&eacute;es de grandeur, Il, un jeu...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01492616f18b4f5b9307f1ed13e8d769",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=21229), Label(value='0 / 21229')))…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e055b2de83f84ddb884e8d8a1c39b7cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=21229), Label(value='0 / 21229')))…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc3205e3d57e41a593f4670a0e3387c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=21229), Label(value='0 / 21229')))…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-21:\n",
      "Process ForkPoolWorker-17:\n",
      "Process ForkPoolWorker-20:\n",
      "Process ForkPoolWorker-18:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/queues.py\", line 352, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-d8b774f51da2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m         ),axis = 1)\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-d8b774f51da2>\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'descr_noun_chunks'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'language'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'fr'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;32melse\u001b[0m \u001b[0mtranslator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'descr_noun_chunks'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         ),axis = 1)\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     df['design_noun_chunks_trs'] = df.parallel_apply(\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandarallel/pandarallel.py\u001b[0m in \u001b[0;36mclosure\u001b[0;34m(data, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    449\u001b[0m                 \u001b[0minput_files\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m                 \u001b[0moutput_files\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m                 \u001b[0mmap_result\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m             )\n\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandarallel/pandarallel.py\u001b[0m in \u001b[0;36mget_workers_result\u001b[0;34m(use_memory_fs, nb_workers, show_progress_bar, nb_columns, queue, chunk_lengths, input_files, output_files, map_result)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinished_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0mmessage_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmessage_type\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mINPUT_FILE_READ\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<string>\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, *args, **kwds)\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/multiprocessing/managers.py\u001b[0m in \u001b[0;36m_callmethod\u001b[0;34m(self, methodname, args, kwds)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethodname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0mkind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'#RETURN'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetbuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"!i\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmaxsize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mremaining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 377, in _make_request\n",
      "    httplib_response = conn.getresponse(buffering=True)\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 377, in _make_request\n",
      "    httplib_response = conn.getresponse(buffering=True)\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 377, in _make_request\n",
      "    httplib_response = conn.getresponse(buffering=True)\n",
      "TypeError: getresponse() got an unexpected keyword argument 'buffering'\n",
      "TypeError: getresponse() got an unexpected keyword argument 'buffering'\n",
      "TypeError: getresponse() got an unexpected keyword argument 'buffering'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/opt/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 44, in mapstar\n",
      "    return list(map(*args))\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/pandarallel/pandarallel.py\", line 59, in global_worker\n",
      "    return _func(x)\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/pandarallel/pandarallel.py\", line 59, in global_worker\n",
      "    return _func(x)\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/pandarallel/pandarallel.py\", line 59, in global_worker\n",
      "    return _func(x)\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/pandarallel/pandarallel.py\", line 143, in wrapper\n",
      "    **kwargs\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/pandarallel/pandarallel.py\", line 143, in wrapper\n",
      "    **kwargs\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/pandarallel/pandarallel.py\", line 143, in wrapper\n",
      "    **kwargs\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/pandarallel/data_types/dataframe.py\", line 31, in worker\n",
      "    return df.apply(func, *args, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/pandarallel/data_types/dataframe.py\", line 31, in worker\n",
      "    return df.apply(func, *args, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/pandarallel/data_types/dataframe.py\", line 31, in worker\n",
      "    return df.apply(func, *args, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\", line 6913, in apply\n",
      "    return op.get_result()\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\", line 6913, in apply\n",
      "    return op.get_result()\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\", line 6913, in apply\n",
      "    return op.get_result()\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/pandas/core/apply.py\", line 186, in get_result\n",
      "    return self.apply_standard()\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/pandas/core/apply.py\", line 186, in get_result\n",
      "    return self.apply_standard()\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/pandas/core/apply.py\", line 186, in get_result\n",
      "    return self.apply_standard()\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/pandas/core/apply.py\", line 285, in apply_standard\n",
      "    values, self.f, axis=self.axis, dummy=dummy, labels=labels\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/pandas/core/apply.py\", line 285, in apply_standard\n",
      "    values, self.f, axis=self.axis, dummy=dummy, labels=labels\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/pandas/core/apply.py\", line 285, in apply_standard\n",
      "    values, self.f, axis=self.axis, dummy=dummy, labels=labels\n",
      "  File \"pandas/_libs/reduction.pyx\", line 651, in pandas._libs.reduction.reduce\n",
      "  File \"pandas/_libs/reduction.pyx\", line 651, in pandas._libs.reduction.reduce\n",
      "  File \"pandas/_libs/reduction.pyx\", line 651, in pandas._libs.reduction.reduce\n",
      "  File \"pandas/_libs/reduction.pyx\", line 153, in pandas._libs.reduction.Reducer.get_result\n",
      "  File \"pandas/_libs/reduction.pyx\", line 153, in pandas._libs.reduction.Reducer.get_result\n",
      "  File \"<ipython-input-16-d8b774f51da2>\", line 20, in <lambda>\n",
      "    else translator.translate(x['descr_noun_chunks'])\n",
      "  File \"pandas/_libs/reduction.pyx\", line 153, in pandas._libs.reduction.Reducer.get_result\n",
      "  File \"<ipython-input-16-d8b774f51da2>\", line 20, in <lambda>\n",
      "    else translator.translate(x['descr_noun_chunks'])\n",
      "  File \"<ipython-input-16-d8b774f51da2>\", line 20, in <lambda>\n",
      "    else translator.translate(x['descr_noun_chunks'])\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/translate/translate.py\", line 42, in translate\n",
      "    return ' '.join(self.provider.get_translation(text_wraped) for text_wraped in text_list)\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/translate/translate.py\", line 42, in translate\n",
      "    return ' '.join(self.provider.get_translation(text_wraped) for text_wraped in text_list)\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/translate/translate.py\", line 42, in translate\n",
      "    return ' '.join(self.provider.get_translation(text_wraped) for text_wraped in text_list)\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/translate/translate.py\", line 42, in <genexpr>\n",
      "    return ' '.join(self.provider.get_translation(text_wraped) for text_wraped in text_list)\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/translate/translate.py\", line 42, in <genexpr>\n",
      "    return ' '.join(self.provider.get_translation(text_wraped) for text_wraped in text_list)\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/translate/translate.py\", line 42, in <genexpr>\n",
      "    return ' '.join(self.provider.get_translation(text_wraped) for text_wraped in text_list)\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/translate/providers/mymemory_translated.py\", line 42, in get_translation\n",
      "    data = self._make_request(text)\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/translate/providers/mymemory_translated.py\", line 42, in get_translation\n",
      "    data = self._make_request(text)\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/translate/providers/mymemory_translated.py\", line 38, in _make_request\n",
      "    response = requests.get(self.base_url, params=params, headers=self.headers)\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/translate/providers/mymemory_translated.py\", line 42, in get_translation\n",
      "    data = self._make_request(text)\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/translate/providers/mymemory_translated.py\", line 38, in _make_request\n",
      "    response = requests.get(self.base_url, params=params, headers=self.headers)\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/requests/api.py\", line 75, in get\n",
      "    return request('get', url, params=params, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/translate/providers/mymemory_translated.py\", line 38, in _make_request\n",
      "    response = requests.get(self.base_url, params=params, headers=self.headers)\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/requests/api.py\", line 60, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/requests/api.py\", line 75, in get\n",
      "    return request('get', url, params=params, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/requests/api.py\", line 75, in get\n",
      "    return request('get', url, params=params, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/requests/api.py\", line 60, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/requests/sessions.py\", line 533, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/requests/api.py\", line 60, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/requests/sessions.py\", line 533, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/requests/sessions.py\", line 646, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/requests/sessions.py\", line 533, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/requests/adapters.py\", line 449, in send\n",
      "    timeout=timeout\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/requests/sessions.py\", line 646, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/requests/sessions.py\", line 646, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 600, in urlopen\n",
      "    chunked=chunked)\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/requests/adapters.py\", line 449, in send\n",
      "    timeout=timeout\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/requests/adapters.py\", line 449, in send\n",
      "    timeout=timeout\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 380, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 600, in urlopen\n",
      "    chunked=chunked)\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 600, in urlopen\n",
      "    chunked=chunked)\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 380, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/opt/anaconda3/lib/python3.7/http/client.py\", line 1336, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 380, in _make_request\n",
      "    httplib_response = conn.getresponse()\n",
      "  File \"/opt/anaconda3/lib/python3.7/http/client.py\", line 1336, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/anaconda3/lib/python3.7/http/client.py\", line 306, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/anaconda3/lib/python3.7/http/client.py\", line 1336, in getresponse\n",
      "    response.begin()\n",
      "  File \"/opt/anaconda3/lib/python3.7/http/client.py\", line 306, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/anaconda3/lib/python3.7/http/client.py\", line 267, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "  File \"/opt/anaconda3/lib/python3.7/http/client.py\", line 306, in begin\n",
      "    version, status, reason = self._read_status()\n",
      "  File \"/opt/anaconda3/lib/python3.7/socket.py\", line 589, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/opt/anaconda3/lib/python3.7/http/client.py\", line 267, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/anaconda3/lib/python3.7/http/client.py\", line 267, in _read_status\n",
      "    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n",
      "  File \"/opt/anaconda3/lib/python3.7/socket.py\", line 589, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/opt/anaconda3/lib/python3.7/socket.py\", line 589, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "def translate(df):\n",
    "    df['designation_trs'] = df.parallel_apply(\n",
    "    lambda x : np.nan if pd.isnull(x['designation']) \n",
    "        else (\n",
    "            x['designation'] if x['language'] == 'fr'\n",
    "            else translator.translate(x['designation'])\n",
    "        ),axis = 1)\n",
    "    \n",
    "    df['description_trs'] = df.parallel_apply(\n",
    "    lambda x : np.nan if pd.isnull(x['description']) \n",
    "        else (\n",
    "            x['description'] if x['language'] == 'fr'\n",
    "            else translator.translate(x['description'])\n",
    "        ),axis = 1)\n",
    "        \n",
    "    df['descr_noun_chunks_trs'] = df.parallel_apply(\n",
    "    lambda x : np.nan if pd.isnull(x['descr_noun_chunks']) \n",
    "        else (\n",
    "            x['descr_noun_chunks'] if x['language'] == 'fr'\n",
    "            else translator.translate(x['descr_noun_chunks'])\n",
    "        ),axis = 1)\n",
    "        \n",
    "    df['design_noun_chunks_trs'] = df.parallel_apply(\n",
    "    lambda x : np.nan if pd.isnull(x['design_noun_chunks']) \n",
    "        else (\n",
    "            x['design_noun_chunks'] if x['language'] == 'fr'\n",
    "            else translator.translate(x['design_noun_chunks'])\n",
    "        ),axis = 1)\n",
    "        \n",
    "translate(df_train)\n",
    "translate(df_test)\n",
    "\n",
    "df_train.to_csv('df_train_preprocessed_full_trs.csv')\n",
    "df_test.to_csv('df_test_preprocessed_full_trs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasts using designation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('df_train_preprocessed_full_2.csv')\n",
    "df_test = pd.read_csv('df_test_preprocessed_full_2.csv')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "len(set(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "df_train['label'] = pd.DataFrame(y_train)\n",
    "df_train = df_train.dropna(subset = ['designation'])\n",
    "y_train = df_train['label']\n",
    "df_train.drop(['label'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_df=0.7,\n",
    "    min_df=0.0001,\n",
    "    strip_accents = 'ascii',\n",
    "    analyzer = 'word')\n",
    "X_train = vectorizer.fit_transform(df_train.designation.values).todense()\n",
    "X_test = vectorizer.transform(df_test.designation.values).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=1000)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pd.DataFrame(X_train_pca).to_csv('X_train_pca_1000.csv')\n",
    "pd.DataFrame(X_test_pca).to_csv('X_test_pca_1000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pca = pd.read_csv('X_train_pca_1000.csv', index_col=0)\n",
    "X_test_pca = pd.read_csv('X_test_pca_1000.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "parameters = {'max_depth':[7], 'n_estimators':[100]}\n",
    "rf = RandomForestClassifier(random_state=0, n_jobs = -1, verbose = 1, max_depth=7, n_estimators=100)\n",
    "\n",
    "cross_val_score(rf, X_train_pca, y=y_train, scoring='f1_weighted', cv=5, n_jobs=-1, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "parameters = {'max_depth':[7, 15], 'n_estimators':[10, 50]}\n",
    "rf = RandomForestClassifier(random_state=0, n_jobs = -1, verbose = 2)\n",
    "\n",
    "clf = RandomizedSearchCV(\n",
    "    rf,\n",
    "    parameters,\n",
    "    scoring = 'f1_weighted',\n",
    "    cv = 3,\n",
    "    n_iter = 4,\n",
    "    return_train_score = True,\n",
    "    verbose = 2,\n",
    "    n_jobs = -1)\n",
    "\n",
    "search = clf.fit(X_train, y_train)\n",
    "\n",
    "warnings.filterwarnings('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame(search.cv_results_)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import xgboost as xgb\n",
    "\n",
    "parameters = {'objective':'multi:softmax',\n",
    "              'n_estimators':[3, 10, 30, 100],\n",
    "              'max_depth': [2, 3, 4]}\n",
    "xgb = xgb.XGBClassifier(verbosity =  2, n_jobs = -1)\n",
    "\n",
    "clf = RandomizedSearchCV(\n",
    "    xgb,\n",
    "    parameters,\n",
    "    scoring = 'f1_weighted',\n",
    "    cv = 5,\n",
    "    n_iter = 5,\n",
    "    return_train_score = True,\n",
    "    n_jobs = -1,\n",
    "    verbose =  2)\n",
    "\n",
    "search = clf.fit(X_train_pca, y_train)\n",
    "evals_result = clf.evals_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame(search.cv_results_)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasts using designation & description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(df_train.description.values)\n",
    "X_test = vectorizer.transform(df_test.description.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Ideas : \n",
    "- Compare performance:\n",
    "    with / wo stop words\n",
    "    with / wo language distinction\n",
    "    using designation / description / chunks, etc.\n",
    "\n",
    "- Translate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
